# Symbolic Gradient Generation

ScalarAutograd now supports symbolic differentiation - generate analytical gradient formulas from mathematical expressions!

## Overview

The symbolic gradient system takes mathematical expressions as text input (using natural operator notation like `c = a + b`) and produces:

1. **Executable gradient code** - Ready-to-run JavaScript/TypeScript functions
2. **Mathematical annotations** - Human-readable formulas as comments
3. **Simplified expressions** - Algebraically optimized for clarity

## Quick Start

### Programmatic API

```typescript
import { parse, computeGradients, simplify, generateGradientCode } from 'scalar-autograd';

// Define your forward pass
const input = `
  a = 3.5
  b = 2.0
  c = a * a + b
  output = sin(c)
`;

// Parse and compute gradients
const program = parse(input);
const gradients = computeGradients(program, ['a', 'b']);

// Simplify and generate code
const simplified = new Map();
for (const [param, expr] of gradients.entries()) {
  simplified.set(param, simplify(expr));
}

const code = generateGradientCode(program, simplified);
console.log(code);
```

**Output:**
```javascript
// Auto-generated gradient computation
// Generated by ScalarAutograd symbolic differentiation

// Forward pass
// a = 3.5
const a = 3.5;

// b = 2
const b = 2;

// c = a^2 + b
const c = a * a + b;

// output = sin(c)
const output = Math.sin(c);

// Gradient computation (reverse-mode autodiff)

// ∂output/∂a = 2 * a * cos(c)
const grad_a = 2 * a * Math.cos(c);

// ∂output/∂b = cos(c)
const grad_b = Math.cos(c);

// Result
const result = {
  value: output,
  gradients: {
    a: grad_a,
    b: grad_b,
  }
};
```

### CLI Tool

```bash
# Simple expression
npx scalar-grad "x = 2; y = 3; output = x*x + y*y" --wrt x,y

# From file
npx scalar-grad --input forward.txt --wrt a,b,c --output gradients.js

# Generate function
npx scalar-grad "z = x*x + y*y; output = sqrt(z)" --format function --function euclideanDistance
```

## Supported Operations

### Arithmetic Operators
- `+` Addition
- `-` Subtraction
- `*` Multiplication
- `/` Division
- `**` Power (exponentiation)

### Mathematical Functions
- **Trigonometric**: `sin`, `cos`, `tan`, `asin`, `acos`, `atan`
- **Hyperbolic**: `sinh`, `cosh`, `tanh`
- **Exponential**: `exp`, `log`
- **Other**: `sqrt`, `abs`, `sign`, `floor`, `ceil`, `round`
- **Two-argument**: `pow`, `min`, `max`

### Vector Operations (Vec2/Vec3)

**Current Support**: Component-wise operations (expand vectors into scalar components)

```typescript
// ✓ WORKS: Component-wise representation
const input = `
  output = vx * vx + vy * vy
`;
// Generates: ∂output/∂vx = 2*vx, ∂output/∂vy = 2*vy

// ✓ WORKS: Vec2/Vec3 constructors parse correctly
const v = parse('v = Vec2(x, y)');  // Parses successfully

// ✓ WORKS: Component access
const comp = parse('a = v.x');  // Parses as VectorAccess node

// ⚠️ LIMITATION: Native vector methods not yet differentiated
// Instead of: output = v.magnitude
// Use: output = sqrt(vx * vx + vy * vy)

// ⚠️ LIMITATION: Dot/cross need expansion
// Instead of: output = u.dot(v)
// Use: output = ux * vx + uy * vy
```

**Recommended Pattern**:
1. Expand vectors into components: `u = (ux, uy)`, `v = (vx, vy)`
2. Write operations using component notation
3. Differentiate w.r.t. each component
4. Result is component-wise gradient (Jacobian)

## Differentiation Rules

The system implements standard calculus rules:

### Basic Rules
- **Constant**: `d/dx(c) = 0`
- **Variable**: `d/dx(x) = 1`, `d/dx(y) = 0`
- **Sum**: `d/dx(u + v) = du/dx + dv/dx`
- **Product**: `d/dx(u * v) = u * dv/dx + v * du/dx`
- **Quotient**: `d/dx(u / v) = (v * du/dx - u * dv/dx) / v²`
- **Power**: `d/dx(u^n) = n * u^(n-1) * du/dx`
- **Chain**: `d/dx(f(g(x))) = f'(g(x)) * g'(x)`

### Transcendental Functions
- `d/dx(sin(x)) = cos(x)`
- `d/dx(cos(x)) = -sin(x)`
- `d/dx(tan(x)) = 1/cos²(x)`
- `d/dx(exp(x)) = exp(x)`
- `d/dx(log(x)) = 1/x`
- `d/dx(sqrt(x)) = 1/(2*sqrt(x))`

## Expression Simplification

The simplifier applies algebraic rules:

### Identity Elimination
- `x + 0 → x`
- `x * 1 → x`
- `x - 0 → x`
- `x / 1 → x`
- `x ** 1 → x`

### Zero Elimination
- `x * 0 → 0`
- `0 / x → 0`
- `x - x → 0`

### Constant Folding
- `2 + 3 → 5`
- `sin(0) → 0`
- `exp(0) → 1`
- `log(1) → 0`

### Special Patterns
- `x + x → 2*x`
- `x * x → x²`
- `-(-x) → x`

## Code Generation Options

### Inline Mode
```typescript
const code = generateGradientCode(program, gradients, {
  includeMath: true,      // Include math notation comments
  varStyle: 'const',      // 'const' | 'let' | 'var'
  includeForward: true,   // Include forward pass
  indent: '  '            // Indentation string
});
```

### Function Mode
```typescript
const func = generateGradientFunction(
  program,
  gradients,
  'computeGradient',      // Function name
  ['x', 'y'],             // Parameters
  { includeMath: true }
);
```

**Output:**
```javascript
/**
 * Compute output and its gradients
 * @param x - Input parameter
 * @param y - Input parameter
 * @returns Object with value and gradients
 */
function computeGradient(x, y) {
  // Forward pass
  // output = x^2 + y^2
  const output = x * x + y * y;

  // Gradient computation
  // ∂output/∂x = 2*x
  const grad_x = 2 * x;

  // ∂output/∂y = 2*y
  const grad_y = 2 * y;

  return {
    value: output,
    gradients: { x: grad_x, y: grad_y }
  };
}
```

## Examples

### Example 1: Euclidean Distance Gradient

```typescript
const input = `
  dx = x1 - x2
  dy = y1 - y2
  dist_sq = dx * dx + dy * dy
  output = sqrt(dist_sq)
`;

const program = parse(input);
const gradients = computeGradients(program, ['x1', 'y1', 'x2', 'y2']);
```

**Generated gradients:**
```javascript
// ∂output/∂x1 = (x1 - x2) / sqrt((x1-x2)² + (y1-y2)²)
const grad_x1 = dx / Math.sqrt(dist_sq);

// ∂output/∂x2 = -(x1 - x2) / sqrt((x1-x2)² + (y1-y2)²)
const grad_x2 = -dx / Math.sqrt(dist_sq);
```

### Example 2: Neural Network Activation

```typescript
const input = `
  z = w * x + b
  output = 1 / (1 + exp(-z))  // sigmoid
`;

const gradients = computeGradients(parse(input), ['w', 'x', 'b']);
```

**Generated gradients:**
```javascript
// ∂sigmoid/∂w = sigmoid(z) * (1 - sigmoid(z)) * x
const grad_w = output * (1 - output) * x;

// ∂sigmoid/∂b = sigmoid(z) * (1 - sigmoid(z))
const grad_b = output * (1 - output);
```

### Example 3: Physics - Kinetic Energy

```typescript
const input = `
  v_sq = vx * vx + vy * vy
  output = 0.5 * m * v_sq
`;

const gradients = computeGradients(parse(input), ['vx', 'vy']);
```

**Generated gradients:**
```javascript
// ∂KE/∂vx = m * vx
const grad_vx = m * vx;

// ∂KE/∂vy = m * vy
const grad_vy = m * vy;
```

### Example 4: Machine Learning - MSE Loss

```typescript
const input = `
  diff = pred - target
  sq_diff = diff * diff
  output = sq_diff
`;

const gradients = computeGradients(parse(input), ['pred']);
```

**Generated gradient:**
```javascript
// ∂MSE/∂pred = 2 * (pred - target)
const grad_pred = 2 * diff;
```

## Comparison with Numerical Autodiff

| Feature | Symbolic | Numerical Autodiff |
|---------|----------|-------------------|
| **Output** | Formula (code) | Runtime values |
| **Speed** | Compile-time | Runtime overhead |
| **Flexibility** | Fixed structure | Dynamic graphs |
| **Readability** | Human-readable | Opaque |
| **Use Case** | Code generation, education | Training, optimization |

## Integration with Existing System

The symbolic system complements the existing runtime autodiff:

```typescript
import { V, parse, computeGradients, generateCode } from 'scalar-autograd';

// 1. Generate symbolic formula
const program = parse('output = x * x + y * y');
const gradients = computeGradients(program, ['x', 'y']);
console.log('Formula:', generateCode(gradients.get('x')));
// Output: Formula: 2 * x

// 2. Verify with numerical autodiff
const x = V.W(3);
const y = V.W(4);
const output = V.add(V.square(x), V.square(y));
output.backward();
console.log('Numerical gradient:', x.grad);
// Output: Numerical gradient: 6  (2 * 3)
```

## Limitations

1. **Vector operations**: Currently limited to component-wise differentiation
2. **Control flow**: No support for conditionals or loops
3. **Arrays**: Single values only, no array operations
4. **Dynamic power**: `u^v` where both vary requires careful handling

## Future Enhancements

- [ ] Higher-order derivatives (Hessian matrices)
- [ ] Jacobian matrices for vector-valued functions
- [ ] LaTeX output for documentation
- [ ] Optimization hints (common subexpression elimination)
- [ ] WebGPU kernel generation
- [ ] Automatic differentiation mode switching

## CLI Reference

```bash
npx scalar-grad [options] [expression]

Options:
  -i, --input <file>       Input file
  -o, --output <file>      Output file (default: stdout)
  --wrt <params>           Parameters (comma-separated)
  --format <type>          js | ts | function | inline
  --no-simplify            Disable simplification
  --function <name>        Function name (for --format=function)
  -h, --help               Show help
  -v, --version            Show version
```

## Contributing

The symbolic system is modular:

- **Parser** (`src/symbolic/Parser.ts`) - Lexer + recursive descent parser
- **AST** (`src/symbolic/AST.ts`) - Node definitions + visitor pattern
- **Differentiation** (`src/symbolic/SymbolicDiff.ts`) - Calculus rules
- **Simplification** (`src/symbolic/Simplify.ts`) - Algebraic rules
- **Code Generation** (`src/symbolic/CodeGen.ts`) - JS/TS output

To add a new operation:

1. Add operator to lexer (if needed)
2. Update parser grammar
3. Add differentiation rule
4. Add simplification rules (optional)
5. Update code generator
6. Add tests

## Resources

- [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)
- [Symbolic Differentiation](https://en.wikipedia.org/wiki/Computer_algebra)
- [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule)
- [Expression Simplification](https://en.wikipedia.org/wiki/Computer_algebra#Simplification)
